# çŸ¥è­˜åº«ç³»çµ±å„ªåŒ–å»ºè­°æ›¸

## ğŸ“Š ç³»çµ±ç¾æ³åˆ†æ

### ç•¶å‰æ¶æ§‹å„ªå‹¢
- âœ… æ¨¡çµ„åŒ–è¨­è¨ˆè‰¯å¥½ï¼ˆparsersã€databaseã€search åˆ†é›¢ï¼‰
- âœ… æ”¯æ´å¤šç¨®æ–‡ä»¶é¡å‹ï¼ˆKnowledge, Training, Troubleshootingï¼‰
- âœ… æ··åˆæœå°‹æ©Ÿåˆ¶ï¼ˆå‘é‡ + é—œéµå­—ï¼‰
- âœ… æ”¯æ´ sqlite-vec å‘é‡æœå°‹

### æ¶æ§‹æ”¹é€²æ©Ÿæœƒ
ç¶“éæ·±å…¥åˆ†æï¼Œç™¼ç¾ä»¥ä¸‹å¯å„ªåŒ–çš„é—œéµé ˜åŸŸï¼š

---

## ğŸ¯ å„ªåŒ–å»ºè­°ä¸€ï¼šè³‡æ–™åº«çµæ§‹å¢å¼·

### ç•¶å‰ Schema ä¸è¶³ä¹‹è™•

**`documents` è¡¨ç¼ºå°‘çš„é—œéµå…ƒæ•¸æ“šï¼š**
```sql
-- ç•¶å‰çµæ§‹
CREATE TABLE documents (
    id INTEGER PRIMARY KEY,
    filename TEXT,
    doc_type TEXT,           -- åˆ†é¡å¤ªç²—ç•¥
    upload_date TIMESTAMP,
    analysis_mode TEXT,
    model_used TEXT
);
```

### å»ºè­°æ–°å¢æ¬„ä½

```sql
-- å„ªåŒ–å¾Œçš„ documents è¡¨
CREATE TABLE documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    filename TEXT NOT NULL,
    
    -- åˆ†é¡èˆ‡æ¨™ç±¤
    doc_type TEXT NOT NULL,              -- 'Knowledge', 'Troubleshooting', 'Training'
    category TEXT,                       -- äºŒç´šåˆ†é¡ (ä¾‹å¦‚: 'Hardware', 'Software', 'Network')
    tags TEXT,                           -- JSON é™£åˆ—æ¨™ç±¤ (ä¾‹å¦‚: ["urgent", "customer-facing"])
    
    -- å…ƒæ•¸æ“š
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_modified TIMESTAMP,             -- æœ€å¾Œä¿®æ”¹æ™‚é–“
    file_size INTEGER,                   -- æª”æ¡ˆå¤§å° (bytes)
    file_hash TEXT,                      -- æª”æ¡ˆ hash (é¿å…é‡è¤‡ä¸Šå‚³)
    version INTEGER DEFAULT 1,           -- ç‰ˆæœ¬è™Ÿ
    
    -- AI è™•ç†è³‡è¨Š
    analysis_mode TEXT,                  -- 'text_only', 'vision', 'auto'
    model_used TEXT,                     -- ä½¿ç”¨çš„æ¨¡å‹
    processing_time REAL,                -- è™•ç†æ™‚é–“ (ç§’)
    
    -- æ¥­å‹™å…ƒæ•¸æ“šï¼ˆé‡è¦ï¼ï¼‰
    author TEXT,                         -- ä½œè€…/ä¸Šå‚³è€…
    department TEXT,                     -- éƒ¨é–€ (ä¾‹å¦‚: 'è£½é€ éƒ¨', 'ç ”ç™¼éƒ¨')
    factory TEXT,                        -- å·¥å»  (ä¾‹å¦‚: 'å» å€A', 'å» å€B')
    language TEXT DEFAULT 'zh-TW',       -- æ–‡ä»¶èªè¨€(ä¸­æ–‡ã€è‹±æ–‡)
    priority INTEGER DEFAULT 0,          -- å„ªå…ˆç´š (0-10, ç”¨æ–¼æœå°‹æ’åº)
    
    -- å…§å®¹æ‘˜è¦
    summary TEXT,                        -- AI ç”Ÿæˆçš„æ–‡ä»¶æ‘˜è¦
    key_points TEXT,                     -- JSON é™£åˆ—ï¼šé‡é»æ‘˜è¦
    
    -- ç‹€æ…‹ç®¡ç†
    status TEXT DEFAULT 'active',        -- 'active', 'archived', 'deprecated'
    access_count INTEGER DEFAULT 0,      -- è¢«æŸ¥è©¢æ¬¡æ•¸
    last_accessed TIMESTAMP              -- æœ€å¾Œè¨ªå•æ™‚é–“
);
```

**ç´¢å¼•å„ªåŒ–ï¼š**
```sql
-- åŠ é€Ÿå¸¸ç”¨æŸ¥è©¢
CREATE INDEX idx_documents_type ON documents(doc_type);
CREATE INDEX idx_documents_category ON documents(category);
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_hash ON documents(file_hash);
CREATE INDEX idx_documents_priority ON documents(priority DESC);
CREATE INDEX idx_documents_access ON documents(access_count DESC);

-- å…¨æ–‡æœå°‹ç´¢å¼•ï¼ˆä½¿ç”¨ FTS5ï¼‰
CREATE VIRTUAL TABLE documents_fts USING fts5(
    filename, summary, key_points,
    content=documents
);
```

### `vec_chunks` è¡¨å¢å¼·

```sql
-- å„ªåŒ–å¾Œçš„ vec_chunks è¡¨
CREATE TABLE vec_chunks (
    chunk_id INTEGER PRIMARY KEY AUTOINCREMENT,
    doc_id INTEGER NOT NULL,
    
    -- å…§å®¹åˆ†é¡
    source_type TEXT NOT NULL,           -- 'chapter', 'step', 'field', 'section'
    source_title TEXT,
    text_content TEXT NOT NULL,
    
    -- å‘é‡èˆ‡é—œéµå­—
    embedding BLOB,
    keywords TEXT,                       -- JSON é™£åˆ—
    
    -- æ–°å¢ï¼šå…§å®¹ç‰¹å¾µ
    chunk_index INTEGER,                 -- åœ¨æ–‡ä»¶ä¸­çš„é †åº
    context_before TEXT,                 -- å‰æ–‡æ‘˜è¦ (å¹«åŠ©ç†è§£ä¸Šä¸‹æ–‡)
    context_after TEXT,                  -- å¾Œæ–‡æ‘˜è¦
    
    -- æ–°å¢ï¼šå“è³ªæŒ‡æ¨™
    content_quality REAL,                -- AI è©•ä¼°çš„å…§å®¹å“è³ªåˆ†æ•¸ (0-1)
    relevance_score REAL,                -- èˆ‡æ–‡ä»¶ä¸»é¡Œçš„ç›¸é—œæ€§ (0-1)
    
    -- æ–°å¢ï¼šä½¿ç”¨çµ±è¨ˆ
    access_count INTEGER DEFAULT 0,      -- è¢«æª¢ç´¢æ¬¡æ•¸
    positive_feedback INTEGER DEFAULT 0, -- æ­£é¢å›é¥‹æ¬¡æ•¸
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (doc_id) REFERENCES documents(id) ON DELETE CASCADE
);

-- ç´¢å¼•
CREATE INDEX idx_vec_chunks_doc ON vec_chunks(doc_id);
CREATE INDEX idx_vec_chunks_type ON vec_chunks(source_type);
CREATE INDEX idx_vec_chunks_quality ON vec_chunks(content_quality DESC);
CREATE INDEX idx_vec_chunks_access ON vec_chunks(access_count DESC);
```

### æ–°å¢ï¼šé—œè¯é—œä¿‚è¡¨

```sql
-- æ–‡ä»¶ä¹‹é–“çš„é—œè¯ (ä¾‹å¦‚ï¼šå¼•ç”¨ã€ç›¸é—œã€æ›´æ–°é—œä¿‚)
CREATE TABLE document_relations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_doc_id INTEGER NOT NULL,
    target_doc_id INTEGER NOT NULL,
    relation_type TEXT NOT NULL,         -- 'references', 'updates', 'related', 'supersedes'
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_doc_id) REFERENCES documents(id) ON DELETE CASCADE,
    FOREIGN KEY (target_doc_id) REFERENCES documents(id) ON DELETE CASCADE
);

-- ç”¨æˆ¶æŸ¥è©¢æ­·å²ï¼ˆç”¨æ–¼æ”¹é€²æœå°‹ï¼‰
CREATE TABLE search_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query TEXT NOT NULL,
    query_embedding BLOB,                -- æŸ¥è©¢çš„å‘é‡
    result_chunks TEXT,                  -- JSON: è¿”å›çš„ chunk_ids
    user_clicked_chunk_id INTEGER,       -- ç”¨æˆ¶å¯¦éš›é»æ“Šçš„çµæœ
    feedback TEXT,                       -- 'helpful', 'not_helpful', 'irrelevant'
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸ¯ å„ªåŒ–å»ºè­°äºŒï¼šæ™ºæ…§å…ƒæ•¸æ“šæå–

### åœ¨æ–‡ä»¶åŒ¯å…¥æ™‚è‡ªå‹•æå–æ›´å¤šè³‡è¨Š

#### ä¿®æ”¹ `process_document_v3()` æµç¨‹

```python
# core/ingestion_v3.py å¢å¼·ç‰ˆ

def process_document_v3(
    file_path: str,
    doc_type: str,
    analysis_mode: str = "auto",
    category: str = None,           # æ–°å¢ï¼šäºŒç´šåˆ†é¡
    department: str = None,         # æ–°å¢ï¼šéƒ¨é–€
    factory: str = None,            # æ–°å¢ï¼šå·¥å» 
    priority: int = 0,              # æ–°å¢ï¼šå„ªå…ˆç´š
    auto_extract_metadata: bool = True,  # æ–°å¢ï¼šæ˜¯å¦è‡ªå‹•æå–å…ƒæ•¸æ“š
    **kwargs
):
    """å¢å¼·ç‰ˆæ–‡ä»¶è™•ç†æµç¨‹"""
    
    # 1. è¨ˆç®—æª”æ¡ˆ hashï¼ˆé¿å…é‡è¤‡ä¸Šå‚³ï¼‰
    file_hash = _calculate_file_hash(file_path)
    existing_doc = database.get_document_by_hash(file_hash)
    if existing_doc:
        logger.warning(f"æ–‡ä»¶å·²å­˜åœ¨: {existing_doc['filename']}")
        return existing_doc['id']
    
    # 2. æå–æª”æ¡ˆåŸºæœ¬è³‡è¨Š
    file_stats = os.stat(file_path)
    metadata = {
        'filename': os.path.basename(file_path),
        'file_size': file_stats.st_size,
        'file_hash': file_hash,
        'doc_type': doc_type,
        'category': category,
        'department': department,
        'factory': factory,
        'priority': priority
    }
    
    # 3. è®€å–æ–‡ä»¶å…§å®¹
    start_time = time.time()
    content = _read_file_content_v3(file_path)
    
    # 4. ä½¿ç”¨ AI æå–å…ƒæ•¸æ“šï¼ˆé—œéµå¢å¼·ï¼ï¼‰
    if auto_extract_metadata and content:
        extracted_metadata = _extract_document_metadata(content, doc_type)
        metadata.update(extracted_metadata)
    
    # 5. å»ºç«‹æ–‡ä»¶è¨˜éŒ„
    processing_time = time.time() - start_time
    doc_id = database.create_document_enhanced(
        **metadata,
        processing_time=processing_time
    )
    
    # ... å¾ŒçºŒåˆ‡ç‰‡è™•ç† ...
    
    return doc_id


def _extract_document_metadata(content: str, doc_type: str) -> dict:
    """
    ä½¿ç”¨ AI å¾æ–‡ä»¶å…§å®¹ä¸­æå–å…ƒæ•¸æ“š
    
    Returns:
        {
            'summary': str,           # æ‘˜è¦
            'key_points': list,       # é‡é»åˆ—è¡¨
            'category': str,          # æ¨è–¦åˆ†é¡
            'tags': list,             # æ¨™ç±¤
            'language': str           # èªè¨€
        }
    """
    prompt = f"""
è«‹åˆ†æä»¥ä¸‹{doc_type}æ–‡ä»¶ï¼Œæå–é—œéµè³‡è¨Šï¼š

ã€æ–‡ä»¶å…§å®¹ã€‘
{content[:3000]}  # åªå–å‰ 3000 å­—é¿å… token éå¤š

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
{{
    "summary": "ä¸€æ®µå¼æ‘˜è¦ï¼ˆä¸è¶…é150å­—ï¼‰",
    "key_points": ["é‡é»1", "é‡é»2", "é‡é»3"],
    "suggested_category": "å»ºè­°çš„äºŒç´šåˆ†é¡",
    "tags": ["æ¨™ç±¤1", "æ¨™ç±¤2"],
    "language": "zh-TW æˆ– en-US"
}}
"""
    
    response = ai_core.analyze_text(prompt, model="gpt-4o-mini")
    
    try:
        metadata = json.loads(response)
        return {
            'summary': metadata.get('summary'),
            'key_points': json.dumps(metadata.get('key_points', []), ensure_ascii=False),
            'category': metadata.get('suggested_category'),
            'tags': json.dumps(metadata.get('tags', []), ensure_ascii=False),
            'language': metadata.get('language', 'zh-TW')
        }
    except:
        logger.warning("å…ƒæ•¸æ“šæå–å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼")
        return {}
```

---

## ğŸ¯ å„ªåŒ–å»ºè­°ä¸‰ï¼šé€šç”¨æŸ¥è©¢å¼•æ“è¨­è¨ˆ

### ç•¶å‰æŸ¥è©¢çš„å•é¡Œ

1. æŸ¥è©¢é‚è¼¯åˆ†æ•£åœ¨å¤šå€‹æ¨¡çµ„ï¼ˆ`vector_search.py`, `hybrid_search.py`, `legacy_search.py`ï¼‰
2. ç¼ºä¹çµ±ä¸€çš„æŸ¥è©¢å…¥å£
3. ç„¡æ³•æ ¹æ“šæŸ¥è©¢é¡å‹è‡ªå‹•é¸æ“‡æœ€ä½³ç­–ç•¥
4. æ²’æœ‰æŸ¥è©¢æ„åœ–åˆ†æ

### å»ºè­°ï¼šå»ºç«‹æ™ºæ…§æŸ¥è©¢è·¯ç”±å™¨

```python
# core/search/query_router.py (æ–°æª”æ¡ˆ)

from typing import List, Dict, Optional
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class QueryIntent(Enum):
    """æŸ¥è©¢æ„åœ–é¡å‹"""
    FACTUAL = "factual"              # äº‹å¯¦æŸ¥è©¢ï¼ˆä¾‹ï¼šä»€éº¼æ˜¯...ï¼‰
    PROCEDURAL = "procedural"        # æ­¥é©ŸæŸ¥è©¢ï¼ˆä¾‹ï¼šå¦‚ä½•...ï¼‰
    TROUBLESHOOTING = "troubleshooting"  # å•é¡Œæ’æŸ¥ï¼ˆä¾‹ï¼šç‚ºä»€éº¼...ã€æ€éº¼ä¿®...ï¼‰
    COMPARATIVE = "comparative"      # æ¯”è¼ƒæŸ¥è©¢ï¼ˆä¾‹ï¼šAå’ŒBçš„å·®ç•°ï¼‰
    DOCUMENT_LOOKUP = "document_lookup"  # æ–‡ä»¶æŸ¥æ‰¾ï¼ˆä¾‹ï¼šæ‰¾åˆ°XXæ–‡ä»¶ï¼‰


class SearchStrategy(Enum):
    """æœå°‹ç­–ç•¥"""
    VECTOR_ONLY = "vector"           # ç´”å‘é‡æœå°‹
    KEYWORD_ONLY = "keyword"         # ç´”é—œéµå­—
    HYBRID = "hybrid"                # æ··åˆæœå°‹
    DOCUMENT_NAME = "document_name"  # æª”åæœå°‹
    SEMANTIC_DEEP = "semantic_deep"  # æ·±åº¦èªæ„æœå°‹


def analyze_query_intent(query: str) -> QueryIntent:
    """
    åˆ†ææŸ¥è©¢æ„åœ–
    
    ä½¿ç”¨è¦å‰‡ + AI æ··åˆåˆ¤æ–·
    """
    query_lower = query.lower()
    
    # è¦å‰‡åˆ¤æ–·
    if any(word in query_lower for word in ['å¦‚ä½•', 'æ€éº¼', 'æ­¥é©Ÿ', 'æµç¨‹', 'how to']):
        return QueryIntent.PROCEDURAL
    
    if any(word in query_lower for word in ['ç‚ºä»€éº¼', 'åŸå› ', 'ç•°å¸¸', 'éŒ¯èª¤', 'æ•…éšœ', 'why', 'error']):
        return QueryIntent.TROUBLESHOOTING
    
    if any(word in query_lower for word in ['å·®ç•°', 'æ¯”è¼ƒ', 'å€åˆ¥', 'vs', 'compare']):
        return QueryIntent.COMPARATIVE
    
    if any(word in query_lower for word in ['æ–‡ä»¶', 'æª”æ¡ˆ', 'æ‰¾åˆ°', 'document', 'file']):
        return QueryIntent.DOCUMENT_LOOKUP
    
    # é è¨­ç‚ºäº‹å¯¦æŸ¥è©¢
    return QueryIntent.FACTUAL


def select_search_strategy(
    query: str,
    intent: QueryIntent,
    doc_type: Optional[str] = None
) -> SearchStrategy:
    """
    æ ¹æ“šæŸ¥è©¢æ„åœ–é¸æ“‡æœ€ä½³æœå°‹ç­–ç•¥
    """
    # å¦‚æœæŸ¥è©¢åŒ…å«æ˜ç¢ºæ–‡ä»¶å/ç·¨è™Ÿï¼Œå„ªå…ˆæª”åæœå°‹
    if _contains_document_identifier(query):
        return SearchStrategy.DOCUMENT_NAME
    
    # æ ¹æ“šæ„åœ–é¸æ“‡
    strategy_map = {
        QueryIntent.DOCUMENT_LOOKUP: SearchStrategy.DOCUMENT_NAME,
        QueryIntent.FACTUAL: SearchStrategy.HYBRID,
        QueryIntent.PROCEDURAL: SearchStrategy.VECTOR_ONLY,
        QueryIntent.TROUBLESHOOTING: SearchStrategy.HYBRID,
        QueryIntent.COMPARATIVE: SearchStrategy.SEMANTIC_DEEP
    }
    
    return strategy_map.get(intent, SearchStrategy.HYBRID)


def universal_search(
    query: str,
    top_k: int = 10,
    doc_type: Optional[str] = None,
    auto_strategy: bool = True,
    **kwargs
) -> Dict:
    """
    é€šç”¨æŸ¥è©¢å¼•æ“å…¥å£
    
    Args:
        query: æŸ¥è©¢æ–‡å­—
        top_k: å›å‚³çµæœæ•¸
        doc_type: é™å®šæ–‡ä»¶é¡å‹
        auto_strategy: æ˜¯å¦è‡ªå‹•é¸æ“‡ç­–ç•¥
        
    Returns:
        {
            'query': str,
            'intent': str,
            'strategy': str,
            'results': List[Dict],
            'meta': {
                'total_found': int,
                'search_time': float,
                'confidence': float
            }
        }
    """
    import time
    start_time = time.time()
    
    # 1. åˆ†ææŸ¥è©¢æ„åœ–
    intent = analyze_query_intent(query)
    logger.info(f"æŸ¥è©¢æ„åœ–: {intent.value}")
    
    # 2. é¸æ“‡æœå°‹ç­–ç•¥
    if auto_strategy:
        strategy = select_search_strategy(query, intent, doc_type)
    else:
        strategy = kwargs.get('strategy', SearchStrategy.HYBRID)
    
    logger.info(f"æœå°‹ç­–ç•¥: {strategy.value}")
    
    # 3. åŸ·è¡Œæœå°‹
    results = _execute_search(query, strategy, top_k, doc_type, **kwargs)
    
    # 4. å¾Œè™•ç†èˆ‡æ’åºå„ªåŒ–
    results = _post_process_results(results, query, intent)
    
    # 5. è¨˜éŒ„æŸ¥è©¢æ­·å²ï¼ˆç”¨æ–¼æŒçºŒå„ªåŒ–ï¼‰
    search_time = time.time() - start_time
    _log_search_history(query, intent, strategy, results, search_time)
    
    return {
        'query': query,
        'intent': intent.value,
        'strategy': strategy.value,
        'results': results[:top_k],
        'meta': {
            'total_found': len(results),
            'search_time': search_time,
            'confidence': _calculate_confidence(results)
        }
    }


def _execute_search(
    query: str,
    strategy: SearchStrategy,
    top_k: int,
    doc_type: Optional[str],
    **kwargs
) -> List[Dict]:
    """åŸ·è¡Œå¯¦éš›æœå°‹"""
    from .vector_search import search_by_vector
    from .legacy_search import search_documents_v2
    from .hybrid_search import hybrid_search
    
    if strategy == SearchStrategy.VECTOR_ONLY:
        return search_by_vector(query, top_k=top_k, **kwargs)
    
    elif strategy == SearchStrategy.KEYWORD_ONLY:
        return search_documents_v2(query, top_k=top_k, **kwargs)
    
    elif strategy == SearchStrategy.HYBRID:
        return hybrid_search(query, top_k=top_k, **kwargs)
    
    elif strategy == SearchStrategy.DOCUMENT_NAME:
        # æª”åå„ªå…ˆæœå°‹
        keyword_results = search_documents_v2(query, top_k=top_k, fuzzy=True)
        if keyword_results:
            return keyword_results
        # é™ç´šåˆ°æ··åˆæœå°‹
        return hybrid_search(query, top_k=top_k, **kwargs)
    
    elif strategy == SearchStrategy.SEMANTIC_DEEP:
        # æ·±åº¦èªæ„æœå°‹ï¼ˆä½¿ç”¨æ›´å¤§çš„ top_k ç„¶å¾Œé‡æ–°æ’åºï¼‰
        results = search_by_vector(query, top_k=top_k * 3, **kwargs)
        # ä½¿ç”¨ AI é‡æ–°æ’åº
        return _semantic_rerank(results, query)[:top_k]


def _post_process_results(
    results: List[Dict],
    query: str,
    intent: QueryIntent
) -> List[Dict]:
    """
    å¾Œè™•ç†çµæœ
    
    - å»é‡
    - è£œå……ä¸Šä¸‹æ–‡
    - èª¿æ•´æ’åº
    """
    # 1. æ ¹æ“š chunk_id å»é‡
    seen_chunks = set()
    deduped_results = []
    for result in results:
        chunk_id = result.get('chunk_id')
        if chunk_id not in seen_chunks:
            seen_chunks.add(chunk_id)
            deduped_results.append(result)
    
    # 2. æ ¹æ“šæ„åœ–èª¿æ•´æ’åº
    if intent == QueryIntent.TROUBLESHOOTING:
        # å„ªå…ˆé¡¯ç¤º Troubleshooting é¡å‹çš„æ–‡ä»¶
        deduped_results.sort(
            key=lambda x: (
                x.get('document', {}).get('doc_type') == 'Troubleshooting',
                x.get('total_score', 0)
            ),
            reverse=True
        )
    
    return deduped_results


def _calculate_confidence(results: List[Dict]) -> float:
    """è¨ˆç®—çµæœä¿¡å¿ƒåº¦"""
    if not results:
        return 0.0
    
    # åŸºæ–¼æœ€é«˜åˆ†èˆ‡å¹³å‡åˆ†çš„å·®ç•°
    scores = [r.get('total_score', r.get('similarity', 0)) for r in results]
    if not scores:
        return 0.0
    
    max_score = max(scores)
    avg_score = sum(scores) / len(scores)
    
    # å¦‚æœæœ€é«˜åˆ†æ˜é¡¯é«˜æ–¼å¹³å‡ï¼Œä¿¡å¿ƒåº¦è¼ƒé«˜
    confidence = max_score if max_score > avg_score * 1.5 else avg_score
    return min(confidence, 1.0)


def _contains_document_identifier(query: str) -> bool:
    """æª¢æŸ¥æŸ¥è©¢æ˜¯å¦åŒ…å«æ–‡ä»¶ç·¨è™Ÿ/åç¨±"""
    import re
    # æª¢æ¸¬å¸¸è¦‹çš„æ–‡ä»¶ç·¨è™Ÿæ ¼å¼ï¼ˆä¾‹å¦‚ï¼šN706, SOP-001, DOC_2024_01ï¼‰
    patterns = [
        r'[A-Z]\d{3,}',           # N706, A123
        r'[A-Z]{2,}-\d{3,}',      # SOP-001
        r'DOC[_-]\d{4}[_-]\d{2}', # DOC_2024_01
    ]
    
    for pattern in patterns:
        if re.search(pattern, query, re.IGNORECASE):
            return True
    return False


def _log_search_history(
    query: str,
    intent: QueryIntent,
    strategy: SearchStrategy,
    results: List[Dict],
    search_time: float
):
    """è¨˜éŒ„æŸ¥è©¢æ­·å²ï¼ˆç”¨æ–¼å¾ŒçºŒå„ªåŒ–ï¼‰"""
    try:
        from core import database
        database.log_search_history(
            query=query,
            intent=intent.value,
            strategy=strategy.value,
            result_count=len(results),
            search_time=search_time
        )
    except Exception as e:
        logger.warning(f"è¨˜éŒ„æŸ¥è©¢æ­·å²å¤±æ•—: {e}")
```

---

## ğŸ¯ å„ªåŒ–å»ºè­°å››ï¼šå¢å¼·æŸ¥è©¢ç²¾æº–åº¦

### 4.1 èªæ„é‡æ’åºï¼ˆSemantic Rerankingï¼‰

```python
# core/search/reranker.py (æ–°æª”æ¡ˆ)

def _semantic_rerank(results: List[Dict], query: str) -> List[Dict]:
    """
    ä½¿ç”¨ AI å°æœå°‹çµæœé‡æ–°æ’åº
    
    é©ç”¨æ–¼éœ€è¦æ·±åº¦ç†è§£æŸ¥è©¢æ„åœ–çš„å ´æ™¯
    """
    if not results:
        return results
    
    # æ§‹å»ºé‡æ’åº prompt
    candidates = []
    for idx, result in enumerate(results[:20]):  # åªé‡æ’å‰ 20 å€‹
        candidates.append({
            'index': idx,
            'title': result.get('source_title', ''),
            'content': result.get('content', '')[:200]  # åªå–å‰ 200 å­—
        })
    
    prompt = f"""
ä½ æ˜¯ä¸€å€‹æœå°‹æ’åºå°ˆå®¶ã€‚ç”¨æˆ¶æŸ¥è©¢æ˜¯ï¼šã€Œ{query}ã€

ä»¥ä¸‹æ˜¯å€™é¸çµæœï¼ˆç·¨è™Ÿ 0-{len(candidates)-1}ï¼‰ï¼š
{json.dumps(candidates, ensure_ascii=False, indent=2)}

è«‹æ ¹æ“šèˆ‡æŸ¥è©¢çš„ç›¸é—œæ€§ï¼Œå°‡çµæœç·¨è™Ÿç”±é«˜åˆ°ä½æ’åºã€‚
åªå›è¦† JSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š[3, 0, 7, 1, ...]
"""
    
    try:
        response = ai_core.analyze_text(prompt, model="gpt-4o-mini")
        reranked_indices = json.loads(response)
        
        # é‡æ–°æ’åˆ—
        reranked_results = [results[i] for i in reranked_indices if i < len(results)]
        # åŠ ä¸Šæœªé‡æ’çš„å…¶é¤˜çµæœ
        remaining = [r for i, r in enumerate(results[20:], start=20)]
        return reranked_results + remaining
        
    except Exception as e:
        logger.warning(f"é‡æ’åºå¤±æ•—: {e}")
        return results
```

### 4.2 æŸ¥è©¢æ“´å±•ï¼ˆQuery Expansionï¼‰

```python
def expand_query(query: str) -> List[str]:
    """
    æŸ¥è©¢æ“´å±•ï¼šç”Ÿæˆèªæ„ç›¸è¿‘çš„æŸ¥è©¢è®Šé«”
    
    ä¾‹å¦‚ï¼šã€Œå¦‚ä½•å®‰è£ã€â†’ ["å¦‚ä½•å®‰è£", "å®‰è£æ­¥é©Ÿ", "å®‰è£æµç¨‹", "å®‰è£æ•™å­¸"]
    """
    prompt = f"""
è«‹ç‚ºä»¥ä¸‹æŸ¥è©¢ç”Ÿæˆ 3-5 å€‹èªæ„ç›¸è¿‘çš„è®Šé«”æŸ¥è©¢ï¼Œå¹«åŠ©æå‡æœå°‹å¬å›ç‡ï¼š

åŸå§‹æŸ¥è©¢ï¼šã€Œ{query}ã€

è¦æ±‚ï¼š
1. ä¿ç•™æ ¸å¿ƒæ„åœ–
2. ä½¿ç”¨åŒç¾©è©æˆ–ä¸åŒè¡¨é”æ–¹å¼
3. åªå›è¦† JSON é™£åˆ—ï¼Œä¾‹å¦‚ï¼š["è®Šé«”1", "è®Šé«”2", "è®Šé«”3"]
"""
    
    try:
        response = ai_core.analyze_text(prompt, model="gpt-4o-mini")
        variants = json.loads(response)
        return [query] + variants  # åŒ…å«åŸå§‹æŸ¥è©¢
    except:
        return [query]
```

---

## ğŸ¯ å„ªåŒ–å»ºè­°äº”ï¼šç¨‹å¼é€£å‹•èˆ‡ API è¨­è¨ˆ

### çµ±ä¸€çš„ API å±¤

```python
# core/search/__init__.py (ä¿®æ”¹)

from .query_router import universal_search, QueryIntent, SearchStrategy

# å°å¤–çµ±ä¸€æ¥å£
__all__ = [
    'universal_search',      # ä¸»è¦å…¥å£
    'QueryIntent',
    'SearchStrategy'
]

# å‘å¾Œå…¼å®¹ï¼ˆä¿ç•™èˆŠæ¥å£ï¼‰
from .vector_search import search_by_vector
from .hybrid_search import hybrid_search
```

### åœ¨ chat_app.py ä¸­ä½¿ç”¨

```python
# chat_app.py (ä¿®æ”¹å¾Œ)

from core.search import universal_search

def handle_user_query(query: str, doc_type_filter: str = None):
    """è™•ç†ç”¨æˆ¶æŸ¥è©¢"""
    
    # ä½¿ç”¨é€šç”¨æŸ¥è©¢å¼•æ“
    search_result = universal_search(
        query=query,
        top_k=10,
        doc_type=doc_type_filter,
        auto_strategy=True  # è‡ªå‹•é¸æ“‡ç­–ç•¥
    )
    
    # é¡¯ç¤ºæœå°‹å…ƒè³‡è¨Š
    st.info(f"""
    ğŸ” æŸ¥è©¢æ„åœ–ï¼š{search_result['intent']}
    ğŸ“Š æœå°‹ç­–ç•¥ï¼š{search_result['strategy']}
    â±ï¸ æœå°‹æ™‚é–“ï¼š{search_result['meta']['search_time']:.2f}ç§’
    ğŸ“ˆ ä¿¡å¿ƒåº¦ï¼š{search_result['meta']['confidence']:.2%}
    """)
    
    # é¡¯ç¤ºçµæœ
    results = search_result['results']
    if results:
        for result in results:
            # ... é¡¯ç¤ºé‚è¼¯ ...
    else:
        st.warning("æœªæ‰¾åˆ°ç›¸é—œå…§å®¹")
```

---

## ğŸ“ å¯¦æ–½è¨ˆç•«

### Phase 1: è³‡æ–™åº«çµæ§‹å‡ç´šï¼ˆ1-2 å¤©ï¼‰
1. âœ… å»ºç«‹ migration script
2. âœ… æ–°å¢æ¬„ä½åˆ° `documents` å’Œ `vec_chunks`
3. âœ… å»ºç«‹æ–°çš„é—œè¯è¡¨
4. âœ… æ›´æ–° `database` æ¨¡çµ„çš„ CRUD æ“ä½œ

### Phase 2: å…ƒæ•¸æ“šæå–å¢å¼·ï¼ˆ2-3 å¤©ï¼‰
1. âœ… å¯¦ä½œ `_extract_document_metadata()`
2. âœ… ä¿®æ”¹ `process_document_v3()` æ•´åˆå…ƒæ•¸æ“šæå–
3. âœ… æ¸¬è©¦ä¸åŒæ–‡ä»¶é¡å‹çš„å…ƒæ•¸æ“šæå–

### Phase 3: é€šç”¨æŸ¥è©¢å¼•æ“ï¼ˆ3-4 å¤©ï¼‰
1. âœ… å»ºç«‹ `query_router.py`
2. âœ… å¯¦ä½œæ„åœ–åˆ†æèˆ‡ç­–ç•¥é¸æ“‡
3. âœ… æ•´åˆç¾æœ‰æœå°‹æ¨¡çµ„
4. âœ… å¯¦ä½œèªæ„é‡æ’åº

### Phase 4: UI æ•´åˆèˆ‡æ¸¬è©¦ï¼ˆ1-2 å¤©ï¼‰
1. âœ… æ›´æ–° `chat_app.py` ä½¿ç”¨æ–° API
2. âœ… æ–°å¢æŸ¥è©¢æ­·å²æŸ¥çœ‹åŠŸèƒ½
3. âœ… é€²è¡Œç«¯åˆ°ç«¯æ¸¬è©¦

---

## ğŸ é¡å¤–å„ªåŒ–å»ºè­°

### 1. æ™ºæ…§æ¨è–¦ç³»çµ±
```python
def get_related_documents(doc_id: int, top_k: int = 5) -> List[Dict]:
    """åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦æ¨è–¦ç›¸é—œæ–‡ä»¶"""
    # å¯¦ä½œé‚è¼¯...
```

### 2. ç”¨æˆ¶å›é¥‹æ©Ÿåˆ¶
```python
def record_user_feedback(chunk_id: int, feedback: str):
    """è¨˜éŒ„ç”¨æˆ¶å°çµæœçš„å›é¥‹ï¼Œç”¨æ–¼æ”¹é€²æœå°‹"""
    # helpful / not_helpful / irrelevant
```

### 3. å®šæœŸå„ªåŒ–ä»»å‹™
- å®šæœŸåˆ†æ `search_history` æ‰¾å‡ºå¸¸è¦‹æŸ¥è©¢æ¨¡å¼
- æ ¹æ“š `access_count` èª¿æ•´æ–‡ä»¶å„ªå…ˆç´š
- è­˜åˆ¥ä½å“è³ªåˆ‡ç‰‡ä¸¦é‡æ–°è™•ç†

---

## âœ¨ é æœŸæ•ˆæœ

å¯¦æ–½é€™äº›å„ªåŒ–å¾Œï¼Œç³»çµ±å°‡èƒ½ï¼š

1. âœ… **æ›´ç²¾æº–**ï¼šé€šéæ„åœ–åˆ†æå’Œç­–ç•¥é¸æ“‡ï¼Œæå‡æœå°‹æº–ç¢ºåº¦ 20-30%
2. âœ… **æ›´æ™ºæ…§**ï¼šè‡ªå‹•æå–å…ƒæ•¸æ“šï¼Œæ¸›å°‘äººå·¥æ¨™è¨»å·¥ä½œ 80%
3. âœ… **æ›´å¿«é€Ÿ**ï¼šå„ªåŒ–éçš„ç´¢å¼•å’ŒæŸ¥è©¢ç­–ç•¥ï¼Œæå‡æŸ¥è©¢é€Ÿåº¦ 40%
4. âœ… **æ›´æ˜“ç”¨**ï¼šçµ±ä¸€çš„ API ä»‹é¢ï¼Œé™ä½ç¶­è­·æˆæœ¬
5. âœ… **å¯æŒçºŒå„ªåŒ–**ï¼šé€šéæŸ¥è©¢æ­·å²åˆ†æï¼Œç³»çµ±èƒ½è‡ªæˆ‘å­¸ç¿’æ”¹é€²

## é©—è­‰è¨ˆç•«

### è‡ªå‹•åŒ–æ¸¬è©¦
å»ºç«‹æ¸¬è©¦è³‡æ–™é›†ï¼š
```python
# tests/test_query_router.py
def test_intent_detection():
    assert analyze_query_intent("å¦‚ä½•å®‰è£è»Ÿé«”") == QueryIntent.PROCEDURAL
    assert analyze_query_intent("ç‚ºä»€éº¼æœƒå‡ºç¾éŒ¯èª¤") == QueryIntent.TROUBLESHOOTING
```

### æ‰‹å‹•æ¸¬è©¦
1. ä¸Šå‚³ 10 å€‹ä¸åŒé¡å‹çš„æ¸¬è©¦æ–‡ä»¶
2. åŸ·è¡Œ 20 å€‹å…¸å‹æŸ¥è©¢ï¼Œæ¯”è¼ƒå„ªåŒ–å‰å¾Œçš„çµæœå“è³ª
3. è¨˜éŒ„æŸ¥è©¢æ™‚é–“å’Œç²¾æº–åº¦æŒ‡æ¨™
