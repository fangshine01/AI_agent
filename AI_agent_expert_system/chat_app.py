"""
AI Expert System - Chat UI (ä½¿ç”¨è€…å•ç­”ä»‹é¢)
Port: 8502

åŠŸèƒ½ï¼š
- å°ˆå®¶å•ç­”ï¼ˆæ•´åˆ v2.0 æœå°‹ï¼‰
- åˆ†é¡ç¯„åœé¸æ“‡
- Session Token çµ±è¨ˆ
"""

import streamlit as st
from datetime import datetime
from core import database, ai_core
from core import search  # v3.0 é‡æ§‹å¾Œçš„ search æ¨¡çµ„
import config
# é é¢è¨­å®š
st.set_page_config(
    page_title="AI Expert System - å°ˆå®¶å•ç­”",
    page_icon="ğŸ’¬",
    layout="wide"
)

# åˆå§‹åŒ– Session State
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'session_tokens' not in st.session_state:
    st.session_state.session_tokens = 0

# æ¨™é¡Œ
st.title("ğŸ’¬ AI Expert System - å°ˆå®¶å•ç­”")
st.caption("ç”± v3.0 å‘é‡æœå°‹å¼•æ“é©…å‹•")

# å´é‚Šæ¬„:API è¨­å®šèˆ‡æœå°‹è¨­å®š
with st.sidebar:
    st.header("ğŸ”‘ API è¨­å®š")
    st.caption("è¼¸å…¥æ‚¨è‡ªå·±çš„ API è³‡è¨Š(å¯é¸)")
    
    # ä½¿ç”¨ session_state å„²å­˜ API è¨­å®š
    if 'user_api_key' not in st.session_state:
        st.session_state.user_api_key = ""
    if 'user_base_url' not in st.session_state:
        st.session_state.user_base_url = "https://api.openai.com/v1"
    
    user_api_key = st.text_input(
        "API Key",
        value=st.session_state.user_api_key,
        type="password",
        help="è«‹è¼¸å…¥æ‚¨çš„ API Key"
    )
    
    user_base_url = st.text_input(
        "Base URL",
        value=st.session_state.user_base_url,
        help="API ç«¯é» URL"
    )
    
    # å„²å­˜åˆ° session_state
    st.session_state.user_api_key = user_api_key
    st.session_state.user_base_url = user_base_url
    
    # é¡¯ç¤ºç‹€æ…‹
    if user_api_key:
        st.success("âœ… API Key å·²è¨­å®š")
    else:
        st.warning("âš ï¸ è«‹è¼¸å…¥ API Key")
    
    st.markdown("---")
    
    st.header("âš™ï¸ æœå°‹è¨­å®š")
    
    # åˆ†é¡éæ¿¾
    selected_types = st.multiselect(
        "æœå°‹ç¯„åœ",
        options=['knowledge', 'training', 'procedure', 'troubleshooting'],
        default=[],
        format_func=lambda x: {
            'knowledge': 'ğŸ“š çŸ¥è­˜åº«',
            'training': 'ğŸ“ æ•™è‚²è¨“ç·´',
            'procedure': 'ğŸ“‹ æ—¥å¸¸æ‰‹é †',
            'troubleshooting': 'ğŸ”§ ç•°å¸¸è§£æ'
        }[x],
        help="é™å®šæœå°‹çš„æ–‡ä»¶é¡å‹ï¼Œç•™ç©ºè¡¨ç¤ºæœå°‹æ‰€æœ‰é¡å‹"
    )
    
    # æœå°‹é™åˆ¶
    search_limit = st.slider("æœå°‹çµæœæ•¸", 1, 20, 5)
    
    # v3.0 æ–°å¢: æœå°‹æ¨¡å¼é¸æ“‡
    st.markdown("**ğŸ”¬ v3.0 æœå°‹æ¨¡å¼**")
    search_mode = st.radio(
        "é¸æ“‡æœå°‹ç­–ç•¥",
        options=["hybrid", "vector", "keyword"],
        format_func=lambda x: {
            "hybrid": "ğŸ”€ æ··åˆæœå°‹ (æ¨è–¦)",
            "vector": "ğŸ¯ å‘é‡æœå°‹",
            "keyword": "ğŸ”¤ é—œéµå­—æœå°‹"
        }[x],
        horizontal=True
    )
    
    # v3.0 æ–°å¢: å•ç­”æ¨¡å‹é¸æ“‡
    st.markdown("**ğŸ¤– å•ç­”æ¨¡å‹**")
    chat_model = st.selectbox(
        "é¸æ“‡æ¨ç†æ¨¡å‹",
        options=["gpt-4o-mini", "gpt-4o", "gemini-2.0-flash-exp"],
        format_func=lambda x: f"{x} {config.MODEL_COST_LABELS.get(x, '')}"
    )
    
    # æ¨¡ç³Šæœå°‹
    enable_fuzzy = st.checkbox("å•Ÿç”¨æ¨¡ç³Šæœå°‹", value=True)
    
    st.markdown("---")
    
    st.header("ğŸ“Š Session ç‹€æ…‹")
    st.metric("æœ¬æ¬¡å°è©± Token", f"{st.session_state.session_tokens:,}")
    
    if st.button("æ¸…ç©ºå°è©±è¨˜éŒ„"):
        st.session_state.messages = []
        st.session_state.session_tokens = 0
        st.rerun()
    
    st.markdown("---")
    st.caption("æç¤ºï¼šä½¿ç”¨æ¨¡ç³Šæœå°‹å¯ä»¥å®¹å¿æ‹¼å¯«éŒ¯èª¤")

# ä¸»è¦å°è©±å€
st.markdown("### å°è©±è¨˜éŒ„")

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # é¡¯ç¤º Token ä½¿ç”¨ï¼ˆå¦‚æœæ˜¯ assistant è¨Šæ¯ï¼‰
        if message["role"] == "assistant" and "tokens" in message:
            st.caption(f"ğŸ’¡ æœ¬æ¬¡ä½¿ç”¨: {message['tokens']} tokens")

# ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # æª¢æŸ¥ API Key
    if not user_api_key:
        st.error("âŒ è«‹å…ˆåœ¨å·¦å´è¨­å®š API Key æ‰èƒ½é€²è¡Œå°è©±")
        st.stop()

    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI å›æ‡‰
    with st.chat_message("assistant"):
        # æª¢æŸ¥æ˜¯å¦ç‚ºåˆ—è¡¨æŸ¥è©¢
        list_query_keywords = ['æœ‰å“ªäº›', 'åˆ—å‡º', 'ç›®éŒ„', 'æ¸…å–®', 'å…¨éƒ¨', 'æ‰€æœ‰æ–‡ä»¶', 'çŸ¥è­˜åº«']
        is_list_query = any(keyword in prompt for keyword in list_query_keywords)
        
        if is_list_query:
            # ç›´æ¥è¿”å›çŸ¥è­˜åº«æ¦‚è¦½
            with st.spinner("æ­£åœ¨æ•´ç†çŸ¥è­˜åº«è³‡è¨Š..."):
                overview = database.get_knowledge_overview()
                
                # ç”ŸæˆçŸ¥è­˜åº«æ¦‚è¦½æ–‡å­—
                response_parts = ["ğŸ“š **çŸ¥è­˜åº«æ¦‚è¦½**\n"]
                response_parts.append(f"ç›®å‰å…±æœ‰ **{overview['total']}** å€‹æ–‡ä»¶\n")
                
                if overview['by_type']:
                    response_parts.append("\n**æ–‡ä»¶é¡å‹çµ±è¨ˆ:**")
                    type_names = {
                        'knowledge': 'çŸ¥è­˜åº«',
                        'training': 'æ•™è‚²è¨“ç·´',
                        'procedure': 'æ—¥å¸¸æ‰‹é †',
                        'troubleshooting': 'ç•°å¸¸è§£æ'
                    }
                    for ftype, count in overview['by_type'].items():
                        response_parts.append(f"- {type_names.get(ftype, ftype)}: {count} å€‹")
                
                if overview['recent_files']:
                    response_parts.append("\n\n**æœ€è¿‘ä¸Šå‚³çš„æ–‡ä»¶:**")
                    for doc in overview['recent_files'][:5]:
                        response_parts.append(f"- {doc['file_name']} ({type_names.get(doc['file_type'], doc['file_type'])})")
                
                if overview['all_keywords']:
                    response_parts.append(f"\n\n**ç†±é–€é—œéµå­—:** {', '.join(overview['all_keywords'][:20])}")
                
                response_parts.append("\n\nğŸ’¡ **ä½¿ç”¨å»ºè­°:** æ‚¨å¯ä»¥è¼¸å…¥ä¸Šè¿°é—œéµå­—æˆ–æ–‡ä»¶åä¾†æŸ¥è©¢å…·é«”å…§å®¹!")
                
                response = "\n".join(response_parts)
                usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 100}
                
                st.markdown(response)
                st.caption("ğŸ’¡ æœ¬æ¬¡ä½¿ç”¨: 100 tokens (åˆ—è¡¨æŸ¥è©¢)")
                
        else:
            # æ­£å¸¸æœå°‹æµç¨‹ (v3.0)
            with st.spinner("æ­£åœ¨æœå°‹ç›¸é—œè³‡æ–™..."):
                # æº–å‚™ API é‡‘é‘°èˆ‡ URL (å„ªå…ˆä½¿ç”¨å´é‚Šæ¬„è¼¸å…¥ï¼Œå¦å‰‡ä½¿ç”¨ç³»çµ±é è¨­)
                api_key_used = user_api_key if user_api_key else None
                base_url_used = user_base_url if user_base_url else None
                
                # 1. æ ¹æ“šæœå°‹æ¨¡å¼é¸æ“‡æœå°‹å‡½æ•¸
                if search_mode == "vector":
                    # ç´”å‘é‡æœå°‹
                    raw_results = search.search_by_vector(
                        query=prompt,
                        top_k=search_limit,
                        api_key=api_key_used,
                        base_url=base_url_used
                    )
                    # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                    search_results = []
                    for r in raw_results:
                        search_results.append({
                            'id': r['doc_id'],
                            'file_name': r['document']['filename'],
                            'file_type': r['document']['doc_type'],
                            'content': r['content'],
                            'similarity': r['similarity']
                        })
                    
                elif search_mode == "hybrid":
                    # æ··åˆæœå°‹
                    raw_results = search.hybrid_search(
                        query=prompt,
                        top_k=search_limit,
                        api_key=api_key_used,
                        base_url=base_url_used
                    )
                    # è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
                    search_results = []
                    for r in raw_results:
                        search_results.append({
                            'id': r['doc_id'],
                            'file_name': r['document']['filename'],
                            'file_type': r['document']['doc_type'],
                            'content': r['content'],
                            'total_score': r['total_score']
                        })
                
                else:
                    # é—œéµå­—æœå°‹ (v2.0)
                    search_results = search.search_documents_v2(
                        query=prompt,
                        file_types=selected_types if selected_types else None,
                        fuzzy=enable_fuzzy,
                        top_k=search_limit
                    )
                
                # 2. æª¢æŸ¥æ˜¯å¦æœ‰æœå°‹çµæœ
                if not search_results:
                    # ç„¡çµæœæ™‚çš„å¾Œå‚™æ©Ÿåˆ¶
                    overview = database.get_knowledge_overview()
                    
                    response_parts = ["æŠ±æ­‰,æˆ‘æ‰¾ä¸åˆ°èˆ‡æ‚¨å•é¡Œç›´æ¥ç›¸é—œçš„è³‡æ–™ã€‚\n"]
                    response_parts.append("**çŸ¥è­˜åº«æ¦‚è¦½:**")
                    response_parts.append(f"- ç›®å‰å…±æœ‰ {overview['total']} å€‹æ–‡ä»¶")
                    
                    if overview['all_keywords']:
                        response_parts.append(f"- å¯æŸ¥è©¢çš„é—œéµå­—: {', '.join(overview['all_keywords'][:15])}")
                    
                    response_parts.append("\nğŸ’¡ **å»ºè­°:**")
                    response_parts.append("1. å˜—è©¦ä½¿ç”¨æ›´ç°¡å–®çš„é—œéµå­—")
                    response_parts.append("2. è¼¸å…¥ã€Œæœ‰å“ªäº›ã€æŸ¥çœ‹å®Œæ•´æ–‡ä»¶ç›®éŒ„")
                    response_parts.append("3. åƒè€ƒä¸Šè¿°é—œéµå­—é‡æ–°æå•")
                    
                    response = "\n".join(response_parts)
                    usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 50}
                    
                    st.markdown(response)
                    st.caption("ğŸ’¡ æœ¬æ¬¡ä½¿ç”¨: 50 tokens (ç„¡çµæœ)")
                    
                else:
                    # æœ‰æœå°‹çµæœ,ç¹¼çºŒæ­£å¸¸æµç¨‹
                    # 2. çµ„åˆä¸Šä¸‹æ–‡ (Context)
                    context_parts = []
                    context_header = "ä»¥ä¸‹æ˜¯ç›¸é—œçš„åƒè€ƒè³‡æ–™:\n\n"
                    

                    for i, doc in enumerate(search_results, 1):
                        # å‹•æ…‹èª¿æ•´ Context é•·åº¦: å¦‚æœçµæœå°‘(é‡å°ç‰¹å®šæ–‡ä»¶), å‰‡æä¾›æ›´å¤šå…§å®¹
                        max_len = 8000 if len(search_results) <= 2 else 3000
                        content = doc.get('raw_content', '')
                        if content:
                            content = content[:max_len]
                        else:
                            content = doc.get('preview', '')
                            
                        context_parts.append(f"[æ–‡ä»¶{i}] {doc['file_name']}\n{content}\n")
                    
                    context = "\n".join(context_parts)
                    
                    # 3. å»ºç«‹ Prompt
                    full_prompt = f"""{context_header}{context}

---

ä½¿ç”¨è€…å•é¡Œ:{prompt}

è«‹æ ¹æ“šä¸Šè¿°åƒè€ƒè³‡æ–™,ç°¡æ½”æ˜ç¢ºåœ°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³,è«‹æ“šå¯¦å‘ŠçŸ¥ã€‚
"""
                    
                    # 4. å‘¼å« AI (å‚³éä½¿ç”¨è€…çš„ API è¨­å®š)
                    with st.spinner("AI æ€è€ƒä¸­..."):
                        # æº–å‚™ API æ‡‰è­‰
                        api_key_used = user_api_key if user_api_key else None
                        base_url_used = user_base_url if user_base_url else None
                        
                        # v3.0: ä½¿ç”¨é¸æ“‡çš„å•ç­”æ¨¡å‹
                        response, usage = ai_core.analyze_slide(
                            text=full_prompt,
                            image_paths=None,
                            api_mode="text_only",
                            api_key=api_key_used,
                            base_url=base_url_used,
                            text_model=chat_model  # v3.0 å‹•æ…‹æ¨¡å‹
                        )
                    # 5. é¡¯ç¤ºå›æ‡‰
                    st.markdown(response)
                    
                    # é¡¯ç¤º Token ä½¿ç”¨
                    tokens_used = usage.get('total_tokens', 0)
                    st.caption(f"ğŸ’¡ æœ¬æ¬¡ä½¿ç”¨: {tokens_used} tokens")
            
                    # é¡¯ç¤ºåƒè€ƒè³‡æ–™ä¾†æº
                    if search_results:
                        # å–å¾—åŒ¹é…å±¤ç´š
                        match_level = search_results[0].get('match_level', 'unknown')
                        match_level_display = {
                            'keywords': 'ğŸ¯ é—œéµå­—',
                            'summary': 'ğŸ“ æ‘˜è¦',
                            'raw_content': 'ğŸ“„ å…¨æ–‡',
                            'unknown': 'ğŸ” ä¸€èˆ¬'
                        }
                        
                        with st.expander(f"ğŸ“š åƒè€ƒè³‡æ–™ä¾†æº ({len(search_results)} ç­†) - åŒ¹é…å±¤ç´š: {match_level_display.get(match_level, 'ğŸ” ä¸€èˆ¬')}"):
                            for doc in search_results:
                                st.write(f"- **{doc['file_name']}** ({doc['file_type']})")
        
                    # è¨˜éŒ„ Token
                    database.log_token_usage(
                        file_name=None,
                        operation='qa',
                        usage=usage
                    )
                    
                    # æ›´æ–° Session çµ±è¨ˆ
                    st.session_state.session_tokens += tokens_used
                    
                    # å„²å­˜è¨Šæ¯
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response,
                        "tokens": tokens_used
                    })
            
                    # é‡æ–°è¼‰å…¥ä»¥æ›´æ–°å´é‚Šæ¬„
                    st.rerun()

# é é¢åº•éƒ¨èªªæ˜
st.markdown("---")
st.caption("ğŸ’¡ æç¤ºï¼š")
st.caption("- æ‚¨å¯ä»¥åœ¨å·¦å´é™å®šæœå°‹ç¯„åœï¼Œä¾‹å¦‚åªæœå°‹ã€Œæ—¥å¸¸æ‰‹é †ã€")
st.caption("- æ¨¡ç³Šæœå°‹å¯ä»¥è‡ªå‹•ä¿®æ­£éŒ¯å­—ï¼Œå¦‚ 'polars' æ‰“æˆ 'polar'")
st.caption("- ç³»çµ±æœƒæ ¹æ“šæ‚¨çš„å•é¡Œï¼Œè‡ªå‹•å¾è³‡æ–™åº«ä¸­æ‰¾å‡ºæœ€ç›¸é—œçš„æ–‡ä»¶ä½œç‚ºåƒè€ƒ")
